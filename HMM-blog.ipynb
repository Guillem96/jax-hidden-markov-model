{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Markov Models (HMM)\n",
    "\n",
    "In this article, we are going to generate new words that statistically sound positive. To do so, we are going to use a set of words gathered from a large number of positive reviews found on the internet [1, 2] and using a Hidden Markov Model (HMM).\n",
    "\n",
    "With HMM we will be able to create a language model which will define how positive words are composed, at least how they are statistically composed.\n",
    "\n",
    "To get things even simpler, we are going to use an existing Python package to work with HMMs developed by my own [3].\n",
    "\n",
    "## What are Markov Chains and HMMs?\n",
    "\n",
    "Markov Chains models the way of moving from a determined state $a$ to another state $b$. Each transition has a probability $p_{ab}$ associated meaning how likely is moving from $a$ to $b$. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state (See Equation 1)[4]. \n",
    "\n",
    "$P(q_i=a|q_{i-1}) $\n",
    "\n",
    "*Equation 1: Markov assumption*\n",
    "\n",
    "Figure 1 shows an example of how a Markov Chain looks like.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Guillem96/jax-hidden-markov-model/master/img/mdp.jpg\" width=400/>\n",
    "\n",
    "*Figure 1. Markov chain (Slide comming from Berkeley Reinforcement Learning course)*\n",
    "\n",
    "The HMM is based on augmenting the Markov chain framework. A Markov chain is useful when we need to compute a probability for a sequence of observable events, but sometimes we cannot see what is happening under the hoods. Imagine, that every day, we see a different animal, for instance, the first day we see an ant, the second day a snail, and finally a white fox (Figure 2). What is happening here? Does seeing an ant modifies the probability a snail the following day? It doesn't, what is modifying the probabilities is the weather. Depending on if it is sunny, rainy or snowy we the probabilities of seeing a specific animal varies.\n",
    "\n",
    "![HMM Animals example](https://raw.githubusercontent.com/Guillem96/jax-hidden-markov-model/master/img/HMM%20Example.png)\n",
    "\n",
    "*Figure 2. HMM animals example*\n",
    "\n",
    "An HMM allows us to talk about both observed events(like the animals) and hidden events (like like the weather) that we think of as causal factors in our probabilistic model.  An HMM is specified bythe following components:\n",
    "\n",
    "- $Q = q_1, q_2, q_3, ...$: The set of possible hidden states (Sunny, rainy and snowy)\n",
    "- $O = o_1, o_2, o_3, ...$: A sequence of **observations** sampled from a vocabulary $V$(White Fox, Snail and Ant)\n",
    "- $\\pi = \\pi_1, \\pi_2, \\pi_3,...$: The probability distribution of starting at a determined hidden state.\n",
    "- $A = a_{ij}$: **Transition probabilities**. Matrix that at position $ij$ we find the probability of going from $q_i$ to $q_j$\n",
    "- $ B = b_i(o_t) $: **Emission probabilities**. Probability of observing $o$ being at state $i$ at timestep $t$\n",
    "\n",
    "As Linus would say: *Talk is cheap, show me code*. And this is exactly what I am going to show you. We are going to learn how to formalize the animals example with my own HMM Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Guillem/Desktop/projects/jax/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import hmm\n",
    "import jax.numpy as np\n",
    "\n",
    "# Decalre the name of the possible Hidden States Q\n",
    "Q_names = ['Sunny', 'Rainy', 'Snowy']\n",
    "\n",
    "# Define the vocabulary of Observations\n",
    "V = ['Ant', 'Snail', 'White Fox']\n",
    "\n",
    "# Define transition probs\n",
    "A = np.array([[0.6, 0.3, 0.1],\n",
    "              [0.4, 0.4, 0.2],\n",
    "              [0.1, 0.4, 0.5]])\n",
    "\n",
    "B = np.array([[0.8, 0.2, 0.0],\n",
    "              [0.1, 0.6, 0.3],\n",
    "              [0.0, 0.1, 0.9]])\n",
    "\n",
    "pi = np.array([.3, .3, .4])\n",
    "\n",
    "animals_hmm = hmm.HiddenMarkovModel(A=A, B=B, pi=pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be at this point the code above is a bin unclear, but if we plot what we have declared the things will become a lot clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Hidden Markov Model Pages: 1 -->\n",
       "<svg width=\"405pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 404.60 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>Hidden Markov Model</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 400.597,-388 400.597,4 -4,4\"/>\n",
       "<!-- q&#45;0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>q&#45;0</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"200.597\" cy=\"-279\" rx=\"33.2948\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.597\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">Sunny</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;q&#45;0 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>q&#45;0&#45;&gt;q&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.543,-286.844C242.121,-287.193 251.745,-284.578 251.745,-279 251.745,-275.165 247.196,-272.731 240.591,-271.697\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.717,-268.199 230.543,-271.156 240.34,-275.189 240.717,-268.199\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.245\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.43</text>\n",
       "</g>\n",
       "<!-- q&#45;1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>q&#45;1</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"89.5975\" cy=\"-192\" rx=\"31.6951\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"89.5975\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rainy</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;q&#45;1 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>q&#45;0&#45;&gt;q&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.176,-263.499C170.868,-253.366 153.905,-239.624 138.597,-228 131.807,-222.843 124.369,-217.42 117.386,-212.423\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.047,-209.31 108.867,-206.374 114.994,-215.017 119.047,-209.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.097\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.32</text>\n",
       "</g>\n",
       "<!-- q&#45;2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>q&#45;2</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"235.597\" cy=\"-105\" rx=\"35.194\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.597\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Snowy</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;q&#45;2 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>q&#45;0&#45;&gt;q&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M209.424,-261.646C221.158,-238.797 241.162,-195.657 247.597,-156 248.836,-148.367 247.863,-140.174 246.032,-132.675\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.355,-131.571 243.132,-122.996 242.65,-133.58 249.355,-131.571\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.097\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.26</text>\n",
       "</g>\n",
       "<!-- o&#45;0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>o&#45;0</title>\n",
       "<ellipse fill=\"#d3f0ce\" stroke=\"#d3f0ce\" cx=\"365.597\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"365.597\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Ant</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;o&#45;0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>q&#45;0&#45;&gt;o&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.904,-278.299C263.361,-276.668 305.32,-269.382 329.597,-243 379.951,-188.28 375.615,-92.4424 369.868,-45.8941\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.333,-45.3974 368.524,-35.9565 366.396,-46.3353 373.333,-45.3974\"/>\n",
       "<text text-anchor=\"middle\" x=\"384.097\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.50</text>\n",
       "</g>\n",
       "<!-- o&#45;1 -->\n",
       "<g id=\"node5\" class=\"node\"><title>o&#45;1</title>\n",
       "<ellipse fill=\"#d3f0ce\" stroke=\"#d3f0ce\" cx=\"28.5975\" cy=\"-18\" rx=\"28.6953\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"28.5975\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Snail</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;o&#45;1 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>q&#45;0&#45;&gt;o&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.989,-267.152C169.613,-265.011 163.949,-262.854 158.597,-261 107.68,-243.356 83.6668,-250.915 48.5975,-210 2.98869,-156.789 21.3959,-123.957 25.5975,-54 25.751,-51.4432 25.9349,-48.7832 26.1351,-46.1228\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"29.6298,-46.3295 26.958,-36.0771 22.6532,-45.758 29.6298,-46.3295\"/>\n",
       "<text text-anchor=\"middle\" x=\"33.0975\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.27</text>\n",
       "</g>\n",
       "<!-- o&#45;2 -->\n",
       "<g id=\"node6\" class=\"node\"><title>o&#45;2</title>\n",
       "<ellipse fill=\"#d3f0ce\" stroke=\"#d3f0ce\" cx=\"235.597\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.597\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">White Fox</text>\n",
       "</g>\n",
       "<!-- q&#45;0&#45;&gt;o&#45;2 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>q&#45;0&#45;&gt;o&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M218.81,-263.774C248.598,-239.059 306.084,-184.873 322.597,-123 326.723,-107.541 329.61,-101.381 322.597,-87 312.272,-65.8245 291.812,-49.3094 273.326,-37.8599\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"274.857,-34.7 264.463,-32.6479 271.309,-40.734 274.857,-34.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"329.097\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.22</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;q&#45;0 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>q&#45;1&#45;&gt;q&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.8191,-210.248C95.6183,-220.914 100.68,-234.201 109.597,-243 123.094,-256.318 142.202,-264.682 159.427,-269.878\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.725,-273.314 169.295,-272.584 160.577,-266.564 158.725,-273.314\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.097\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.35</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;q&#45;1 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>q&#45;1&#45;&gt;q&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.283,-199.827C129.788,-200.269 139.445,-197.66 139.445,-192 139.445,-188.109 134.88,-185.659 128.289,-184.652\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"128.439,-181.156 118.283,-184.173 128.104,-188.148 128.439,-181.156\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.945\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.35</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;q&#45;2 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>q&#45;1&#45;&gt;q&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.565,-175.943C115.904,-165.154 132.272,-150.847 148.597,-141 163.464,-132.033 181.042,-124.463 196.492,-118.682\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.071,-121.834 206.288,-115.145 195.694,-115.25 198.071,-121.834\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.097\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.29</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;o&#45;0 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>q&#45;1&#45;&gt;o&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99.239,-174.615C107.247,-160.905 118.861,-140.788 128.597,-123 145.205,-92.6581 136.331,-73.9363 164.597,-54 211.904,-20.6349 236.699,-46.6628 293.597,-36 305.802,-33.7128 319.058,-30.7286 330.829,-27.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.848,-31.254 340.728,-25.4757 330.183,-24.4549 331.848,-31.254\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.097\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.26</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;o&#45;1 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>q&#45;1&#45;&gt;o&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.8739,-175.743C64.9713,-162.706 50.3158,-142.941 42.5975,-123 32.9776,-98.1459 29.7804,-67.7375 28.8069,-46.238\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"32.303,-46.0562 28.4837,-36.1737 25.3066,-46.2809 32.303,-46.0562\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.0975\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.43</text>\n",
       "</g>\n",
       "<!-- q&#45;1&#45;&gt;o&#45;2 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>q&#45;1&#45;&gt;o&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M88.6405,-173.827C88.1556,-152.377 89.6972,-115.084 103.597,-87 118.7,-56.4869 153.352,-39.3589 183.363,-29.9263\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.711,-33.1788 193.327,-27.0142 182.747,-26.4599 184.711,-33.1788\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.097\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.32</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;q&#45;0 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>q&#45;2&#45;&gt;q&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232.956,-123.002C230.889,-133.067 227.369,-145.801 221.597,-156 216.178,-165.577 208.859,-163.855 204.597,-174 194.302,-198.512 194.52,-229.204 196.567,-250.874\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.112,-251.471 197.72,-261.012 200.068,-250.68 193.112,-251.471\"/>\n",
       "<text text-anchor=\"middle\" x=\"217.097\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.26</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;q&#45;1 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>q&#45;2&#45;&gt;q&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.448,-121.169C207.46,-131.875 190.353,-146.03 173.597,-156 158.673,-164.88 141.078,-172.587 125.858,-178.491\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.378,-175.308 116.251,-182.105 126.843,-181.86 124.378,-175.308\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.097\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.35</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;q&#45;2 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>q&#45;2&#45;&gt;q&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.214,-112.858C278.996,-113.115 288.694,-110.496 288.694,-105 288.694,-101.221 284.11,-98.8026 277.417,-97.7436\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.402,-94.2368 267.214,-97.1423 276.99,-101.225 277.402,-94.2368\"/>\n",
       "<text text-anchor=\"middle\" x=\"301.194\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.39</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;o&#45;0 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>q&#45;2&#45;&gt;o&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.484,-90.3433C278.729,-75.7983 314.027,-52.7195 338.422,-36.7686\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"340.418,-39.6454 346.872,-31.2435 336.587,-33.7866 340.418,-39.6454\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.097\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.22</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;o&#45;1 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>q&#45;2&#45;&gt;o&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M208.96,-93.0617C171.232,-77.5696 102.288,-49.2594 61.44,-32.4861\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.5222,-29.1469 51.9422,-28.586 59.8632,-35.6222 62.5222,-29.1469\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.097\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.24</text>\n",
       "</g>\n",
       "<!-- q&#45;2&#45;&gt;o&#45;2 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>q&#45;2&#45;&gt;o&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M235.597,-86.799C235.597,-75.1626 235.597,-59.5479 235.597,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.098,-46.1754 235.597,-36.1754 232.098,-46.1755 239.098,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"248.097\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.54</text>\n",
       "</g>\n",
       "<!-- pi -->\n",
       "<g id=\"node7\" class=\"node\"><title>pi</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200.597\" cy=\"-366\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.597\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">pi</text>\n",
       "</g>\n",
       "<!-- pi&#45;&gt;q&#45;0 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>pi&#45;&gt;q&#45;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200.597,-347.799C200.597,-336.163 200.597,-320.548 200.597,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.098,-307.175 200.597,-297.175 197.098,-307.175 204.098,-307.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.097\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.30</text>\n",
       "</g>\n",
       "<!-- pi&#45;&gt;q&#45;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>pi&#45;&gt;q&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.738,-351.816C159.258,-331.508 114.81,-289.969 96.5975,-243 93.8406,-235.89 92.149,-227.838 91.1172,-220.305\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.5768,-219.736 90.0554,-210.155 87.6148,-220.464 94.5768,-219.736\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.097\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.30</text>\n",
       "</g>\n",
       "<!-- pi&#45;&gt;q&#45;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>pi&#45;&gt;q&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.966,-354.982C242.571,-344.002 272.499,-324.253 285.597,-297 315.957,-233.834 294.69,-203.303 262.597,-141 260.621,-137.162 258.209,-133.321 255.649,-129.664\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"258.352,-127.435 249.542,-121.55 252.759,-131.645 258.352,-127.435\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.097\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.40</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fc324477470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals_hmm.draw(Q_names, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot we can see the probabilities ($\\pi$) of starting at specific hidden state, the transition probabilites and the emission probabilities being at each hidden state.\n",
    "\n",
    "That's all for the Hidden Markov Models introduction. If you want to learn more about them, IMO this [4] notes are a pretty good resource.\n",
    "\n",
    "Now lets move on to our use case of creating positive *soundish* words.\n",
    "\n",
    "## Preprocessing corpus\n",
    "\n",
    "Since our goal is to generate new words, we should work at character level, meaning that characters are going to be the observations and the total number of timesteps, is going to be the word length. For example, a word $W_l$ of length $l$ will be composed of $l$ letters $o_1, o_2, ..., o_l$, in other words, $Wl$ is going to be a sequence of $l$ observations.\n",
    "\n",
    "Now imagine that the generated word is \"hello\". In this case, we have:\n",
    "\n",
    "$W_5 = o_1, o_2, ..., o_5$ where \n",
    "\n",
    "$o_1 = h, o_2 = e, o_3 = l, o_4 = l, o_5 = o$\n",
    "\n",
    "Similar to other Natural Language Processing (NLP) tasks, we cannot work using characters neither words, we have to convert them using a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello = 8,5,12,12,15\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "letters = string.ascii_lowercase + '-'\n",
    "letter2idx = {o: i for i, o in enumerate(letters, start=1)} # Reserve 0 for padding\n",
    "\n",
    "print('hello =', ','.join(str(letter2idx[o]) for o in 'hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a way of converting words into numbers, we can load the dataset and convert some random existing words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 2005\n",
      "['humourous', 'appreciatively', 'amply', 'pampers', 'nicer']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open('data/words.txt') as f:\n",
    "    dataset = [w.strip() for w in f.readlines() \n",
    "               if w.strip() and not w.startswith(';')]\n",
    "\n",
    "print('Number of words:', len(dataset))\n",
    "print(random.sample(dataset, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pride = 16,18,9,4,5\n",
      "considerate = 3,15,14,19,9,4,5,18,1,20,5\n",
      "skillful = 19,11,9,12,12,6,21,12\n",
      "elegance = 5,12,5,7,1,14,3,5\n",
      "cute = 3,21,20,5\n"
     ]
    }
   ],
   "source": [
    "def word2idx(word):\n",
    "    return [letter2idx[o] for o in word]\n",
    "\n",
    "random_words = random.sample(dataset, 5)\n",
    "for w in random_words:\n",
    "    print(f'{w} =', ','.join(map(str, word2idx(w))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily convert a word to numbers. Pretty interesting, isn't it? But we want to go a bit further. \n",
    "\n",
    "Usually, Machine Learning (ML) models are trained using *batches* of data, and HMMs are not different. So, before moving forward, we need a function that samples $n$ words and packs them into a single tensor. Also, if you are familiar with sequences, you should now that to pack heterogeneous sequences into a single tensor they must have the same size, in other words, they all need to be of size $n$. To achieve that, we usually pad sequences to match the larger sequence inside the batch, which exactly $n$ elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape [N_SAMPLES, SEQUENCE_LEN]: (32, 13)\n"
     ]
    }
   ],
   "source": [
    "def sampler(batch_size=32, pad_val=0):\n",
    "    batch = random.sample(dataset, batch_size)\n",
    "    max_len = max(len(o) for o in batch)\n",
    "    padded_batch = []\n",
    "    for b in batch:\n",
    "        offset = max_len - len(b)\n",
    "        padded_batch.append(word2idx(b) + [pad_val] * offset)\n",
    "    \n",
    "    return np.array(padded_batch)\n",
    "\n",
    "sample_batch = sampler()\n",
    "print('Batch shape [N_SAMPLES, SEQUENCE_LEN]:', sample_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done here. Let's move on to the training part.\n",
    "\n",
    "## Training an HMM\n",
    "\n",
    "In this section, we are going to train an HMM from scratch using the package seen above. \n",
    "\n",
    "First of all, we are going to declare an HMM having at least eight possible hidden states and having as many observations as the length of our vocabulary.\n",
    "\n",
    "> Note that the number of hidden states is arbitrary. With a bigger number of hidden states, we will tend to higher variances (probably overfit), and with low values for hidden states will have a higher bias (higher probability of underfitting). Summarizing, the number of hidden states is an hyperparameter, and you have to play with it in order to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "words_hmm = hmm.HiddenMarkovModel.random_init(\n",
    "    key, n_hidden_states=64, n_observations=len(letters) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our HMM, what we need to do, is to tweak the parameters ($A$, $B$ and $\\pi$) so they maximize the probability of training words to appear.\n",
    "\n",
    "If we now sample a word, we are going to see that it does not make any sense. This is because HMM is randomly intialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word: qevc-\n"
     ]
    }
   ],
   "source": [
    "def decode_word(indices):\n",
    "    vocab = ['<pad>'] + list(letters)\n",
    "    return ''.join(vocab[i] for i in indices)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "generated_word = words_hmm.sample(subkey, timesteps=5) # Sample a word with 5 characters\n",
    "print('Generated word:', decode_word(generated_word.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create, words with sense, or at least words pretending to have sense, we are going to maximize the probability of training words. To do so, we are going to use Stochastic Gradient Descent (SGD). More precicely, we are going to compute the gradients of the HMM parameters with respect of Negative Log Likelihood (NLL), and substract a small amount of the gradient to our parameters to maximize the training words likelihood.\n",
    "\n",
    "$w_{t+1} = w_t - \\alpha \\frac{\\partial L(w_t)}{\\partial w_t}$\n",
    "\n",
    "where $w$ are the HMM parameters and $L$ is the $NLL$.\n",
    "\n",
    "$NLL = \\frac{\\sum_{n}{-\\log p_i}}{n}$\n",
    "\n",
    "where $p$ is the probability of a given training word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natively, my `hmm` package does not support *batching* with `likelihood` method. Luckily, JAX allow us to vectorize a function by just decorating it with `jax.vmap`. Also, `hmm` module, does not provide a simple way of computing the gradients of the HMM parameters with respect to an error function, but again, JAX provides the decorator `grad` or `value_and_grad` to automatically compute the derivatives of standard python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import hmm.functional as F\n",
    "\n",
    "v_likelihood = jax.vmap(F.likelihood, in_axes=(None, 0))\n",
    "\n",
    "# Computes the NLL given the HMM and training words\n",
    "def forward(hmm, words):\n",
    "    # hmm package works with log probabilities,\n",
    "    # so likelihood method returns the log probability instead of the *standard one*\n",
    "    log_prob = v_likelihood(hmm, words)\n",
    "    # To compute NLL we just have to neg the probability returned from the \n",
    "    # likelihood method\n",
    "    return -log_prob.mean()\n",
    "\n",
    "# Partial derivative of first arg (HMM params)\n",
    "backward = jax.value_and_grad(forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now call `backward`, the function will return us the loss value and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 33.322784\n"
     ]
    }
   ],
   "source": [
    "sample_batch = sampler(batch_size=4)\n",
    "losses, grads = backward(words_hmm, sample_batch)\n",
    "print('Loss:', losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply a simple SGD step with a pretty high learning rate $\\alpha$, we are going to see how the loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 33.311375\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e-1\n",
    "words_hmm = words_hmm - grads * alpha\n",
    "losses, grads = backward(words_hmm, sample_batch)\n",
    "print('Loss:', losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reapeat SGD for 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [9/5000] Loss: 52.9000\n",
      "Step [19/5000] Loss: 49.2192\n",
      "Step [29/5000] Loss: 48.6336\n",
      "Step [39/5000] Loss: 48.4212\n",
      "Step [49/5000] Loss: 48.4863\n",
      "Step [59/5000] Loss: 48.4650\n",
      "Step [69/5000] Loss: 48.6809\n",
      "Step [79/5000] Loss: 48.2119\n",
      "Step [89/5000] Loss: 47.8807\n",
      "Step [99/5000] Loss: 47.5494\n",
      "Step [109/5000] Loss: 47.3936\n",
      "Step [119/5000] Loss: 47.2317\n",
      "Step [129/5000] Loss: 47.1183\n",
      "Step [139/5000] Loss: 47.0642\n",
      "Step [149/5000] Loss: 46.8623\n",
      "Step [159/5000] Loss: 46.7237\n",
      "Step [169/5000] Loss: 46.6001\n",
      "Step [179/5000] Loss: 46.6507\n",
      "Step [189/5000] Loss: 46.6429\n",
      "Step [199/5000] Loss: 46.4572\n",
      "Step [209/5000] Loss: 46.3333\n",
      "Step [219/5000] Loss: 46.1755\n",
      "Step [229/5000] Loss: 46.1553\n",
      "Step [239/5000] Loss: 46.2286\n",
      "Step [249/5000] Loss: 46.1521\n",
      "Step [259/5000] Loss: 46.1531\n",
      "Step [269/5000] Loss: 46.1175\n",
      "Step [279/5000] Loss: 46.1050\n",
      "Step [289/5000] Loss: 46.0712\n",
      "Step [299/5000] Loss: 46.0073\n",
      "Step [309/5000] Loss: 45.9971\n",
      "Step [319/5000] Loss: 45.9569\n",
      "Step [329/5000] Loss: 45.9082\n",
      "Step [339/5000] Loss: 45.8236\n",
      "Step [349/5000] Loss: 45.8143\n",
      "Step [359/5000] Loss: 45.8647\n",
      "Step [369/5000] Loss: 45.8092\n",
      "Step [379/5000] Loss: 45.7897\n",
      "Step [389/5000] Loss: 45.7953\n",
      "Step [399/5000] Loss: 45.7903\n",
      "Step [409/5000] Loss: 45.7251\n",
      "Step [419/5000] Loss: 45.7212\n",
      "Step [429/5000] Loss: 45.7024\n",
      "Step [439/5000] Loss: 45.6630\n",
      "Step [449/5000] Loss: 45.6446\n",
      "Step [459/5000] Loss: 45.6398\n",
      "Step [469/5000] Loss: 45.6266\n",
      "Step [479/5000] Loss: 45.5948\n",
      "Step [489/5000] Loss: 45.5753\n",
      "Step [499/5000] Loss: 45.5740\n",
      "Step [509/5000] Loss: 45.5890\n",
      "Step [519/5000] Loss: 45.5976\n",
      "Step [529/5000] Loss: 45.5766\n",
      "Step [539/5000] Loss: 45.5446\n",
      "Step [549/5000] Loss: 45.5142\n",
      "Step [559/5000] Loss: 45.4555\n",
      "Step [569/5000] Loss: 45.4520\n",
      "Step [579/5000] Loss: 45.4429\n",
      "Step [589/5000] Loss: 45.4182\n",
      "Step [599/5000] Loss: 45.3763\n",
      "Step [609/5000] Loss: 45.3235\n",
      "Step [619/5000] Loss: 45.3372\n",
      "Step [629/5000] Loss: 45.3463\n",
      "Step [639/5000] Loss: 45.3267\n",
      "Step [649/5000] Loss: 45.3413\n",
      "Step [659/5000] Loss: 45.3132\n",
      "Step [669/5000] Loss: 45.2796\n",
      "Step [679/5000] Loss: 45.2691\n",
      "Step [689/5000] Loss: 45.2800\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e-2\n",
    "running_loss = 0\n",
    "training_steps = 5000\n",
    "\n",
    "for step in range(training_steps):\n",
    "    loss, grads = backward(words_hmm, sampler())\n",
    "    words_hmm = words_hmm - grads * alpha\n",
    "    running_loss += loss\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        mean_loss = running_loss / step\n",
    "        print(f'Step [{step}/{training_steps}] Loss: {mean_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "generated_word = words_hmm.sample(subkey, timesteps=5) # Sample a word with 5 characters\n",
    "print('Generated word:', decode_word(generated_word.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "       Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "       Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "       Washington, USA, \n",
    "\n",
    "[2] Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing \n",
    "       and Comparing Opinions on the Web.\" Proceedings of the 14th \n",
    "       International World Wide Web conference (WWW-2005), May 10-14, \n",
    "       2005, Chiba, Japan.\n",
    "\n",
    "[3] Guillem96 - [Implementation of Hidden Markov Models using JAX](https://github.com/Guillem96/jax-hidden-markov-model)\n",
    "\n",
    "[4] Daniel Jurafsky & James H. Martin - [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/A.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
